# Building Clustering Models to study unstructured data
This project is in response to the Titanic Machine Learning Project from [Kaggle](https://www.kaggle.com/competitions/titanic/overview). I highly recommend anyone who had just started out with machine learning to try out the project. My codes are well commented and structured, so it should be easy to follow my thought patterns as I work through the project.

## ðŸŽ¯ My Objectives
* To share my work
* Receive feedback on improving my methods
* Serve as reference material for anyone trying to learn how to use Machine Learning Models to performe clustering using Scikitlearn. 

## ðŸ““ Project Description 
* **Objective** - To understand the unstructured data by reduce its dimensionality and clustering them using the most suitable clustering algorithm.
* **Problem Statement** - Perform EDA on data, build different clustering models, compare their performance metrics.
* **Data Source** - Provided by Kaggle from the project [page](https://www.kaggle.com/competitions/titanic/data)

## ðŸ““ Data Dictionary
- Survived - Survival (0 = No, 1 = Yes)
- Pclass - Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)
- Name - Name of passenger
- Sex - Male or female
- Age - Age of passenger
- SibSp - # of siblings/spouses aboard the Titanic
- Parch - # of parents/children aboard the TItanic
- Ticket - Ticket number
- Fare - Passenger Fare
- Cabin - Cabin Number
- Embarked - Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)

## ðŸ““Key Features Used
* Libraries used - Numpy, Pandas, Scipy, Matplotlib, Seaborn, Scikitlearn, Tensorflow, Keras, Dask 
* Software used - Python programming, Google Colab

## ðŸ““ Techniques used
* **Cleaning data** from irrelevant data, missing data, duplicated data and outliers
* **Data visualization** using seaborn plots
* **Exploratory Data Analysis** (Univariate and multivariate)
* **Data Preprocessing** by **Feature Engineering**, one-hot encoding, standardizing and scaling data before training algorithms
* **Building Deep Learning Model** using KerasClassifiers in a sequential model
* **Hyperparameter Tuning**
 * Number of layers and neurons
 * Number of epochs
 * Optimizer type
 * Learning Rate
 * Batch Size
 * Dropout Rate
 * Batch Normalization epsilon and momentum
 * AUC-ROC threshold
* **Hyperparameter Tuners**
 * Dask Tuner
 * Keras Tuner
 * Grid Search CV
 * Random Search CV

## ðŸ““ Usage Guide

**For Learners**
1. Download the dataset from the linked sources, and the .ipynb file from the repository.
2. Open the .ipynb file using Google Colaboratory or Jupyter notebook.
3. Read through the project context, objective, data description and dictionary.
4. Try and run the code in the .ipynb notebook, cell by cell. As the codes are well-commented, try to understand what each line of code does.
5. Use my code as a reference if you're stucked, or kindly contact me to knwo more ðŸ˜ƒ

**For Non-technical Learners**
1. Open up the .ipynb or .HTML file to understand into details what was done in the project.
2. Kindly contact me if you want to know more ðŸ˜ƒ

**Thanks for reaching the end!**

